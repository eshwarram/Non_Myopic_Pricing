{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to find the Stackelberg Equilibria of a Pricing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from operator import add, neg\n",
    "\n",
    "\n",
    "\n",
    "def precise_stackelberg_equilibrium_old(leader_payoff_matrix, follower_payoff_matrix):\n",
    "  num_leader_actions = leader_payoff_matrix.shape[0]\n",
    "  num_follower_actions = leader_payoff_matrix.shape[1]\n",
    "\n",
    "  # Define the constraint matrix A_ub and the right-hand side b_ub\n",
    "  A_ub = np.zeros((num_follower_actions + 2, num_leader_actions))\n",
    "  b_ub = np.zeros(num_follower_actions + 2)\n",
    "  bounds = [(0, 1) for _ in range(num_leader_actions)]\n",
    "  best_leader_payoff = np.min(leader_payoff_matrix)\n",
    "  best_leader_strategy = np.zeros(num_leader_actions)\n",
    "\n",
    "  for benchmark_follower_action in range(num_follower_actions):\n",
    "    row_index = 0\n",
    "    c_leader = -leader_payoff_matrix[:, benchmark_follower_action]  # maximize leader's payoff when follower plays a particular action\n",
    "    for i in range(num_follower_actions):\n",
    "      A_ub[row_index] = follower_payoff_matrix.T[i] - follower_payoff_matrix.T[benchmark_follower_action]\n",
    "      b_ub[row_index] = 0\n",
    "      row_index = row_index + 1\n",
    "\n",
    "    A_ub[row_index] = np.ones(num_leader_actions)\n",
    "    b_ub[row_index] = 1\n",
    "    row_index = row_index + 1\n",
    "    A_ub[row_index] = -1 * np.ones(num_leader_actions)\n",
    "    b_ub[row_index] = -1\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    result = linprog(c_leader, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "    print(result.status)\n",
    "\n",
    "    # Extract the solution\n",
    "    leader_optimal_strategy = result.x\n",
    "\n",
    "    # compare different LPs for different optimizer actions\n",
    "    follower_benchmark_distribution = np.zeros(num_follower_actions)\n",
    "    follower_benchmark_distribution[benchmark_follower_action] = 1\n",
    "    leader_payoff = evaluate_leader_payoff(leader_payoff_matrix, leader_optimal_strategy, follower_benchmark_distribution)\n",
    "    if (leader_payoff >= best_leader_payoff):\n",
    "      best_leader_payoff = leader_payoff\n",
    "      best_leader_strategy = leader_optimal_strategy\n",
    "      follower_response = benchmark_follower_action\n",
    "  return best_leader_payoff, best_leader_strategy, follower_response\n",
    "\n",
    "\n",
    "def precise_stackelberg_value(leader_payoff_matrix, follower_payoff_matrix):\n",
    "    (val,_,_) = precise_stackelberg_equilibrium(leader_payoff_matrix, follower_payoff_matrix)\n",
    "    return val\n",
    "\n",
    "# code for computing mnse from https://github.com/sid230798/Game_Theory/blob/master/Problem3/analyse_equilibrium.py\n",
    "def msne(a):\n",
    "    a = a.T\n",
    "    ## One zero array for later (z, x)\n",
    "    ess = np.ones(a.shape[0]+1)\n",
    "    ess[0] = 0\n",
    "\n",
    "    c = -1*(1-ess)  ##[-1, 0 ,0 ,0] -1 coeff for z and 0 for x (Max z == min(-z))\n",
    "    A_ub = np.concatenate((np.ones((1, a.shape[1])), -1*a), axis=0).T\n",
    "    B_ub = np.zeros(a.shape[1])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    B_eq = np.ones(1)\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[0]\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p1_val, p1_distribution = result.x[0], result.x[1:]\n",
    "\n",
    "    ## For 2nd player distribution\n",
    "    ess = np.ones(a.shape[1]+1)\n",
    "    ess[0] = 0\n",
    "    c = (1-ess)\n",
    "    A_ub = np.concatenate((-1*np.ones((a.shape[0], 1)), a), axis=1)\n",
    "    B_ub = np.zeros(a.shape[0])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    A_eq = np.concatenate((A_eq, 1-A_eq), axis=0)\n",
    "    B_eq = np.array([1, p1_val]) ## Dual Principle w* = z*\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[1]\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p2_val, p2_distribution = result.x[0], result.x[1:]\n",
    "\n",
    "    print(\"MSNE are : {\", tuple(p1_distribution), \",\" ,tuple(p2_distribution), \"}\")\n",
    "\n",
    "def maxmin(a):\n",
    "    a = a.T\n",
    "    print(a)\n",
    "    ## One zero array for later (z, x)\n",
    "    ess = np.ones(a.shape[0]+1)\n",
    "    ess[0] = 0\n",
    "\n",
    "    c = -1*(1-ess)  ##[-1, 0 ,0 ,0] -1 coeff for z and 0 for x (Max z == min(-z))\n",
    "    A_ub = np.concatenate((np.ones((1, a.shape[1])), -1*a), axis=0).T\n",
    "    B_ub = np.zeros(a.shape[1])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    B_eq = np.ones(1)\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[0]\n",
    "    print(\"data:\")\n",
    "    print(c)\n",
    "    print(A_ub)\n",
    "    print(B_ub)\n",
    "    print(A_eq)\n",
    "    print(B_eq)\n",
    "    print(bounds)\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p1_distribution = result.x[1:]\n",
    "    return p1_distribution\n",
    "\n",
    "def transform_game_matrix(game_matrix, mixed_strategies):\n",
    "    \"\"\"\n",
    "    Transforms each game matrix based on the set of mixed strategies. Each mixed strategy\n",
    "    becomes a new 'pure' strategy in the transformed games for the followers.\n",
    "    \"\"\"\n",
    "    # Convert the distributions list into a NumPy array for easier manipulation\n",
    "    distributions_array = np.array(mixed_strategies)\n",
    "    transformed_matrix = np.dot(game_matrix.T, distributions_array.T).T\n",
    "    return transformed_matrix\n",
    "\n",
    "\n",
    "def evaluate_leader_payoff_old(game_matrix, leader_strategy, follower_strategy):\n",
    "    \"\"\"\n",
    "    Evaluates the leader's expected payoff on a certain follower game given the leader's strategy and the follower's strategy.\n",
    "    \"\"\"\n",
    "    leader_strategy = np.array(leader_strategy)\n",
    "    follower_strategy = np.array(follower_strategy)\n",
    "    expected_payoff = 0\n",
    "    for leader_strategy, leader_prob in enumerate(leader_strategy):\n",
    "        for follower_action in range(len(follower_strategy)):\n",
    "          expected_payoff += leader_prob * follower_strategy[follower_action] * game_matrix[leader_strategy][follower_action]\n",
    "    return expected_payoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_leader_payoff(game_matrix, leader_strategy, follower_strategy):\n",
    "    \"\"\"\n",
    "    Evaluates the leader's expected payoff given the leader's strategy and the follower's strategy.\n",
    "    \n",
    "    Parameters:\n",
    "    game_matrix (2D numpy array): The leader's payoff matrix\n",
    "    leader_strategy (1D numpy array): The leader's mixed strategy\n",
    "    follower_strategy (1D numpy array): The follower's mixed strategy\n",
    "    \n",
    "    Returns:\n",
    "    float: The expected payoff for the leader\n",
    "    \"\"\"\n",
    "    leader_strategy = np.array(leader_strategy)\n",
    "    follower_strategy = np.array(follower_strategy)\n",
    "    \n",
    "    expected_payoff = 0.0\n",
    "    \n",
    "    for i in range(len(leader_strategy)):\n",
    "        for j in range(len(follower_strategy)):\n",
    "            # Probability of this outcome\n",
    "            prob = leader_strategy[i] * follower_strategy[j]\n",
    "            \n",
    "            # Payoff for the leader in this outcome\n",
    "            payoff = game_matrix[i][j]\n",
    "            \n",
    "            # Add to the expected payoff\n",
    "            expected_payoff += prob * payoff\n",
    "    \n",
    "    return expected_payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precise_stackelberg_equilibrium(leader_payoff_matrix, follower_payoff_matrix):\n",
    "    num_leader_actions = leader_payoff_matrix.shape[0]\n",
    "    num_follower_actions = leader_payoff_matrix.shape[1]\n",
    "\n",
    "    print(f\"Number of leader actions: {num_leader_actions}\")\n",
    "    print(f\"Number of follower actions: {num_follower_actions}\")\n",
    "\n",
    "    # Define the constraint matrix A_ub and the right-hand side b_ub\n",
    "    A_ub = np.zeros((num_follower_actions + 2, num_leader_actions))\n",
    "    b_ub = np.zeros(num_follower_actions + 2)\n",
    "    bounds = [(0, 1) for _ in range(num_leader_actions)]\n",
    "    best_leader_payoff = float('-inf')  # Initialize to negative infinity\n",
    "    best_leader_strategy = np.zeros(num_leader_actions)\n",
    "    follower_response = None\n",
    "\n",
    "    for benchmark_follower_action in range(num_follower_actions):\n",
    "        print(f\"\\nChecking follower action {benchmark_follower_action}\")\n",
    "        row_index = 0\n",
    "        c_leader = -leader_payoff_matrix[:, benchmark_follower_action]  # maximize leader's payoff when follower plays a particular action\n",
    "        for i in range(num_follower_actions):\n",
    "            A_ub[row_index] = follower_payoff_matrix.T[i] - follower_payoff_matrix.T[benchmark_follower_action]\n",
    "            b_ub[row_index] = 0\n",
    "            row_index += 1\n",
    "\n",
    "        A_ub[row_index] = np.ones(num_leader_actions)\n",
    "        b_ub[row_index] = 1\n",
    "        row_index += 1\n",
    "        A_ub[row_index] = -1 * np.ones(num_leader_actions)\n",
    "        b_ub[row_index] = -1\n",
    "\n",
    "        print(\"Linear programming setup:\")\n",
    "        print(f\"c_leader: {c_leader}\")\n",
    "        print(f\"A_ub:\\n{A_ub}\")\n",
    "        print(f\"b_ub: {b_ub}\")\n",
    "        print(f\"bounds: {bounds}\")\n",
    "\n",
    "        # Solve the linear programming problem\n",
    "        result = linprog(c_leader, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "        print(f\"LP result status: {result.status}\")\n",
    "\n",
    "        if result.success:\n",
    "            # Extract the solution\n",
    "            leader_optimal_strategy = result.x\n",
    "            print(f\"Leader optimal strategy: {leader_optimal_strategy}\")\n",
    "\n",
    "            # compare different LPs for different optimizer actions\n",
    "            follower_benchmark_distribution = np.zeros(num_follower_actions)\n",
    "            follower_benchmark_distribution[benchmark_follower_action] = 1\n",
    "            leader_payoff = np.dot(leader_optimal_strategy, leader_payoff_matrix[:, benchmark_follower_action])\n",
    "            print(f\"Leader payoff: {leader_payoff}\")\n",
    "\n",
    "            if leader_payoff > best_leader_payoff:\n",
    "                best_leader_payoff = leader_payoff\n",
    "                best_leader_strategy = leader_optimal_strategy\n",
    "                follower_response = benchmark_follower_action\n",
    "        else:\n",
    "            print(f\"Linear programming failed for follower action {benchmark_follower_action}\")\n",
    "\n",
    "    print(f\"\\nFinal results:\")\n",
    "    print(f\"Best leader payoff: {best_leader_payoff}\")\n",
    "    print(f\"Best leader strategy: {best_leader_strategy}\")\n",
    "    print(f\"Follower response: {follower_response}\")\n",
    "\n",
    "    return best_leader_payoff, best_leader_strategy, follower_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leader actions: 2\n",
      "Number of follower actions: 2\n",
      "\n",
      "Checking follower action 0\n",
      "Linear programming setup:\n",
      "c_leader: [-3 -5]\n",
      "A_ub:\n",
      "[[ 0.  0.]\n",
      " [ 2.  1.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 2\n",
      "Linear programming failed for follower action 0\n",
      "\n",
      "Checking follower action 1\n",
      "Linear programming setup:\n",
      "c_leader: [ 0 -1]\n",
      "A_ub:\n",
      "[[-2. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [-0.  1.]\n",
      "Leader payoff: 1.0\n",
      "\n",
      "Final results:\n",
      "Best leader payoff: 1.0\n",
      "Best leader strategy: [-0.  1.]\n",
      "Follower response: 1\n"
     ]
    }
   ],
   "source": [
    "#Prisoners dilemma game matrix\n",
    "\n",
    "leader_matrix = np.array([[3, 0],\n",
    "              [5, 1]])\n",
    "follower_matrix = np.array([[3, 5],\n",
    "              [0, 1]])\n",
    "\n",
    "best_leader_payoff, best_leader_strategy, follower_response = precise_stackelberg_equilibrium(leader_matrix, follower_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leader actions: 2\n",
      "Number of follower actions: 2\n",
      "\n",
      "Checking follower action 0\n",
      "Linear programming setup:\n",
      "c_leader: [-3  0]\n",
      "A_ub:\n",
      "[[ 0.  0.]\n",
      " [-2.  3.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [1. 0.]\n",
      "Leader payoff: 3.0\n",
      "\n",
      "Checking follower action 1\n",
      "Linear programming setup:\n",
      "c_leader: [ 0 -2]\n",
      "A_ub:\n",
      "[[ 2. -3.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [-0.  1.]\n",
      "Leader payoff: 2.0\n",
      "\n",
      "Final results:\n",
      "Best leader payoff: 3.0\n",
      "Best leader strategy: [1. 0.]\n",
      "Follower response: 0\n",
      "Battle of the Sexes (with a twist)\n",
      "Best leader payoff: 3.0\n",
      "Best leader strategy: [1. 0.]\n",
      "Follower response: 0\n",
      "Expected: Payoff 3, Strategy [1, 0], Response 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Battle of the Sexes (with a twist)\n",
    "leader_matrix = np.array([[3, 0],\n",
    "                          [0, 2]])\n",
    "follower_matrix = np.array([[2, 0],\n",
    "                            [0, 3]])\n",
    "\n",
    "best_leader_payoff, best_leader_strategy, follower_response = precise_stackelberg_equilibrium(leader_matrix, follower_matrix)\n",
    "\n",
    "print(\"Battle of the Sexes (with a twist)\")\n",
    "print(f\"Best leader payoff: {best_leader_payoff}\")\n",
    "print(f\"Best leader strategy: {best_leader_strategy}\")\n",
    "print(f\"Follower response: {follower_response}\")\n",
    "print(f\"Expected: Payoff 3, Strategy [1, 0], Response 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leader actions: 2\n",
      "Number of follower actions: 2\n",
      "\n",
      "Checking follower action 0\n",
      "Linear programming setup:\n",
      "c_leader: [-3 -4]\n",
      "A_ub:\n",
      "[[ 0.  0.]\n",
      " [ 1. -1.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [-0.  1.]\n",
      "Leader payoff: 4.0\n",
      "\n",
      "Checking follower action 1\n",
      "Linear programming setup:\n",
      "c_leader: [-2 -1]\n",
      "A_ub:\n",
      "[[-1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [1. 0.]\n",
      "Leader payoff: 2.0\n",
      "\n",
      "Final results:\n",
      "Best leader payoff: 4.0\n",
      "Best leader strategy: [-0.  1.]\n",
      "Follower response: 0\n",
      "Chicken Game\n",
      "Best leader payoff: 4.0\n",
      "Best leader strategy: [-0.  1.]\n",
      "Follower response: 0\n",
      "Expected: Payoff 4, Strategy [0, 1], Response 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Chicken Game\n",
    "leader_matrix = np.array([[3, 2],\n",
    "                          [4, 1]])\n",
    "follower_matrix = np.array([[3, 4],\n",
    "                            [2, 1]])\n",
    "\n",
    "best_leader_payoff, best_leader_strategy, follower_response = precise_stackelberg_equilibrium(leader_matrix, follower_matrix)\n",
    "\n",
    "print(\"Chicken Game\")\n",
    "print(f\"Best leader payoff: {best_leader_payoff}\")\n",
    "print(f\"Best leader strategy: {best_leader_strategy}\")\n",
    "print(f\"Follower response: {follower_response}\")\n",
    "print(f\"Expected: Payoff 4, Strategy [0, 1], Response 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of leader actions: 2\n",
      "Number of follower actions: 2\n",
      "\n",
      "Checking follower action 0\n",
      "Linear programming setup:\n",
      "c_leader: [-1 -3]\n",
      "A_ub:\n",
      "[[ 0.  0.]\n",
      " [-3.  2.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [0.4 0.6]\n",
      "Leader payoff: 2.2\n",
      "\n",
      "Checking follower action 1\n",
      "Linear programming setup:\n",
      "c_leader: [1 3]\n",
      "A_ub:\n",
      "[[ 3. -2.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [-1. -1.]]\n",
      "b_ub: [ 0.  0.  1. -1.]\n",
      "bounds: [(0, 1), (0, 1)]\n",
      "LP result status: 0\n",
      "Leader optimal strategy: [0.4 0.6]\n",
      "Leader payoff: -2.2\n",
      "\n",
      "Final results:\n",
      "Best leader payoff: 2.2\n",
      "Best leader strategy: [0.4 0.6]\n",
      "Follower response: 0\n",
      "Inspector Game (properly corrected)\n",
      "Best leader payoff: 2.2\n",
      "Best leader strategy: [0.4 0.6]\n",
      "Follower response: 0\n",
      "Expected: Payoff 2.2, Strategy [0.4, 0.6], Response 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Inspection Game \n",
    "leader_matrix = np.array([[1, -1],\n",
    "                          [3, -3]])\n",
    "follower_matrix = np.array([[0, -3],\n",
    "                            [-1, 1]])\n",
    "\n",
    "best_leader_payoff, best_leader_strategy, follower_response = precise_stackelberg_equilibrium(leader_matrix, follower_matrix)\n",
    "\n",
    "print(\"Inspector Game (properly corrected)\")\n",
    "print(f\"Best leader payoff: {best_leader_payoff}\")\n",
    "print(f\"Best leader strategy: {best_leader_strategy}\")\n",
    "print(f\"Follower response: {follower_response}\")\n",
    "print(f\"Expected: Payoff 2.2, Strategy [0.4, 0.6], Response 0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
