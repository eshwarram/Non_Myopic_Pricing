{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to find the Stackelberg Equilibria of a Pricing Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from operator import add, neg\n",
    "\n",
    "\n",
    "\n",
    "def precise_stackelberg_equilibrium(leader_payoff_matrix, follower_payoff_matrix):\n",
    "  num_leader_actions = leader_payoff_matrix.shape[0]\n",
    "  num_follower_actions = leader_payoff_matrix.shape[1]\n",
    "\n",
    "  # Define the constraint matrix A_ub and the right-hand side b_ub\n",
    "  A_ub = np.zeros((num_follower_actions + 2, num_leader_actions))\n",
    "  b_ub = np.zeros(num_follower_actions + 2)\n",
    "  bounds = [(0, 1) for _ in range(num_leader_actions)]\n",
    "  best_leader_payoff = np.min(leader_payoff_matrix)\n",
    "  best_leader_strategy = np.zeros(num_leader_actions)\n",
    "\n",
    "  for benchmark_follower_action in range(num_follower_actions):\n",
    "    row_index = 0\n",
    "    c_leader = -leader_payoff_matrix[:, benchmark_follower_action]  # maximize leader's payoff when follower plays a particular action\n",
    "    for i in range(num_follower_actions):\n",
    "      A_ub[row_index] = follower_payoff_matrix.T[i] - follower_payoff_matrix.T[benchmark_follower_action]\n",
    "      b_ub[row_index] = 0\n",
    "      row_index = row_index + 1\n",
    "\n",
    "    A_ub[row_index] = np.ones(num_leader_actions)\n",
    "    b_ub[row_index] = 1\n",
    "    row_index = row_index + 1\n",
    "    A_ub[row_index] = -1 * np.ones(num_leader_actions)\n",
    "    b_ub[row_index] = -1\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    result = linprog(c_leader, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n",
    "    print(result.status)\n",
    "\n",
    "    # Extract the solution\n",
    "    leader_optimal_strategy = result.x\n",
    "\n",
    "    # compare different LPs for different optimizer actions\n",
    "    follower_benchmark_distribution = np.zeros(num_follower_actions)\n",
    "    follower_benchmark_distribution[benchmark_follower_action] = 1\n",
    "    leader_payoff = evaluate_leader_payoff(leader_payoff_matrix, leader_optimal_strategy, follower_benchmark_distribution)\n",
    "    if (leader_payoff >= best_leader_payoff):\n",
    "      best_leader_payoff = leader_payoff\n",
    "      best_leader_strategy = leader_optimal_strategy\n",
    "      follower_response = benchmark_follower_action\n",
    "  return best_leader_payoff, best_leader_strategy, follower_response\n",
    "\n",
    "\n",
    "def precise_stackelberg_value(leader_payoff_matrix, follower_payoff_matrix):\n",
    "    (val,_,_) = precise_stackelberg_equilibrium(leader_payoff_matrix, follower_payoff_matrix)\n",
    "    return val\n",
    "\n",
    "# code for computing mnse from https://github.com/sid230798/Game_Theory/blob/master/Problem3/analyse_equilibrium.py\n",
    "def msne(a):\n",
    "    a = a.T\n",
    "    ## One zero array for later (z, x)\n",
    "    ess = np.ones(a.shape[0]+1)\n",
    "    ess[0] = 0\n",
    "\n",
    "    c = -1*(1-ess)  ##[-1, 0 ,0 ,0] -1 coeff for z and 0 for x (Max z == min(-z))\n",
    "    A_ub = np.concatenate((np.ones((1, a.shape[1])), -1*a), axis=0).T\n",
    "    B_ub = np.zeros(a.shape[1])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    B_eq = np.ones(1)\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[0]\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p1_val, p1_distribution = result.x[0], result.x[1:]\n",
    "\n",
    "    ## For 2nd player distribution\n",
    "    ess = np.ones(a.shape[1]+1)\n",
    "    ess[0] = 0\n",
    "    c = (1-ess)\n",
    "    A_ub = np.concatenate((-1*np.ones((a.shape[0], 1)), a), axis=1)\n",
    "    B_ub = np.zeros(a.shape[0])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    A_eq = np.concatenate((A_eq, 1-A_eq), axis=0)\n",
    "    B_eq = np.array([1, p1_val]) ## Dual Principle w* = z*\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[1]\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p2_val, p2_distribution = result.x[0], result.x[1:]\n",
    "\n",
    "    print(\"MSNE are : {\", tuple(p1_distribution), \",\" ,tuple(p2_distribution), \"}\")\n",
    "\n",
    "def maxmin(a):\n",
    "    a = a.T\n",
    "    print(a)\n",
    "    ## One zero array for later (z, x)\n",
    "    ess = np.ones(a.shape[0]+1)\n",
    "    ess[0] = 0\n",
    "\n",
    "    c = -1*(1-ess)  ##[-1, 0 ,0 ,0] -1 coeff for z and 0 for x (Max z == min(-z))\n",
    "    A_ub = np.concatenate((np.ones((1, a.shape[1])), -1*a), axis=0).T\n",
    "    B_ub = np.zeros(a.shape[1])\n",
    "    A_eq = np.expand_dims(ess, axis=0)\n",
    "    B_eq = np.ones(1)\n",
    "    bounds = [(None, None)] + [(0,1)]*a.shape[0]\n",
    "    print(\"data:\")\n",
    "    print(c)\n",
    "    print(A_ub)\n",
    "    print(B_ub)\n",
    "    print(A_eq)\n",
    "    print(B_eq)\n",
    "    print(bounds)\n",
    "    result = linprog(c, A_ub=A_ub, b_ub=B_ub, A_eq=A_eq, b_eq=B_eq, bounds=bounds)\n",
    "    p1_distribution = result.x[1:]\n",
    "    return p1_distribution\n",
    "\n",
    "def transform_game_matrix(game_matrix, mixed_strategies):\n",
    "    \"\"\"\n",
    "    Transforms each game matrix based on the set of mixed strategies. Each mixed strategy\n",
    "    becomes a new 'pure' strategy in the transformed games for the followers.\n",
    "    \"\"\"\n",
    "    # Convert the distributions list into a NumPy array for easier manipulation\n",
    "    distributions_array = np.array(mixed_strategies)\n",
    "    transformed_matrix = np.dot(game_matrix.T, distributions_array.T).T\n",
    "    return transformed_matrix\n",
    "\n",
    "\n",
    "def evaluate_leader_payoff(game_matrix, leader_strategy, follower_strategy):\n",
    "    \"\"\"\n",
    "    Evaluates the leader's expected payoff on a certain follower game given the leader's strategy and the follower's strategy.\n",
    "    \"\"\"\n",
    "    leader_strategy = np.array(leader_strategy)\n",
    "    follower_strategy = np.array(follower_strategy)\n",
    "    expected_payoff = 0\n",
    "    for leader_strategy, leader_prob in enumerate(leader_strategy):\n",
    "        for follower_action in range(len(follower_strategy)):\n",
    "          expected_payoff += leader_prob * follower_strategy[follower_action] * game_matrix[leader_strategy][follower_action]\n",
    "    return expected_payoff\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m leader_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m               [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m      5\u001b[0m follower_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m      6\u001b[0m               [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m----> 8\u001b[0m best_leader_payoff, best_leader_strategy, follower_response \u001b[38;5;241m=\u001b[39m precise_stackelberg_equilibrium(leader_matrix, follower_matrix)\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mprecise_stackelberg_equilibrium\u001b[0;34m(leader_payoff_matrix, follower_payoff_matrix)\u001b[0m\n\u001b[1;32m     41\u001b[0m follower_benchmark_distribution \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_follower_actions)\n\u001b[1;32m     42\u001b[0m follower_benchmark_distribution[benchmark_follower_action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 43\u001b[0m leader_payoff \u001b[38;5;241m=\u001b[39m evaluate_leader_payoff(leader_payoff_matrix, leader_optimal_strategy, follower_benchmark_distribution)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (leader_payoff \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m best_leader_payoff):\n\u001b[1;32m     45\u001b[0m   best_leader_payoff \u001b[38;5;241m=\u001b[39m leader_payoff\n",
      "Cell \u001b[0;32mIn[1], line 128\u001b[0m, in \u001b[0;36mevaluate_leader_payoff\u001b[0;34m(game_matrix, leader_strategy, follower_strategy)\u001b[0m\n\u001b[1;32m    126\u001b[0m follower_strategy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(follower_strategy)\n\u001b[1;32m    127\u001b[0m expected_payoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m leader_strategy, leader_prob \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(leader_strategy):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m follower_action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(follower_strategy)):\n\u001b[1;32m    130\u001b[0m       expected_payoff \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m leader_prob \u001b[38;5;241m*\u001b[39m follower_strategy[follower_action] \u001b[38;5;241m*\u001b[39m game_matrix[leader_strategy][follower_action]\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "#Prisoners dilemma game matrix\n",
    "\n",
    "leader_matrix = np.array([[3, 0],\n",
    "              [5, 1]])\n",
    "follower_matrix = np.array([[3, 5],\n",
    "              [0, 1]])\n",
    "\n",
    "best_leader_payoff, best_leader_strategy, follower_response = precise_stackelberg_equilibrium(leader_matrix, follower_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
